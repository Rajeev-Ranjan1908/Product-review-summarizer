{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_json(\"Cell_Phones_and_Accessories.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760450, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rev_verify</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>IC</th>\n",
       "      <th>Prod_meta</th>\n",
       "      <th>Reviewer_Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rev_summ</th>\n",
       "      <th>Review_timestamp</th>\n",
       "      <th>Useful</th>\n",
       "      <th>Prod_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>09 1, 2015</td>\n",
       "      <td>B009XD5TPQ</td>\n",
       "      <td>None</td>\n",
       "      <td>Sunny  Zoeller</td>\n",
       "      <td>Bought it for my husband. He's very happy with it</td>\n",
       "      <td>He's very happy with</td>\n",
       "      <td>1441065600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>01 9, 2016</td>\n",
       "      <td>B016MF3P3K</td>\n",
       "      <td>None</td>\n",
       "      <td>Denise Lesley</td>\n",
       "      <td>Great screen protector.  Doesn't even seem as ...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1452297600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>01 14, 2016</td>\n",
       "      <td>B00IJJCQBA</td>\n",
       "      <td>{'Color:': ' Black / Black'}</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>To tight on my phone and the bottom piece was ...</td>\n",
       "      <td>One Star</td>\n",
       "      <td>1452729600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>06 21, 2017</td>\n",
       "      <td>B00NIJOGOG</td>\n",
       "      <td>{'Color:': ' Rose Gold [6+] CHOOSE CORRECT SIZ...</td>\n",
       "      <td>SG</td>\n",
       "      <td>Very good and superior quality, looks great. M...</td>\n",
       "      <td>Very nice and good quality!!!</td>\n",
       "      <td>1498003200</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>06 10, 2015</td>\n",
       "      <td>B00MQYS97Y</td>\n",
       "      <td>None</td>\n",
       "      <td>Linda</td>\n",
       "      <td>The charger is not working, however ! The comp...</td>\n",
       "      <td>Not happy</td>\n",
       "      <td>1433894400</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating  Rev_verify  Review_Date          IC  \\\n",
       "0          5        True   09 1, 2015  B009XD5TPQ   \n",
       "1          5        True   01 9, 2016  B016MF3P3K   \n",
       "10         1        True  01 14, 2016  B00IJJCQBA   \n",
       "100        5       False  06 21, 2017  B00NIJOGOG   \n",
       "1000       3        True  06 10, 2015  B00MQYS97Y   \n",
       "\n",
       "                                              Prod_meta   Reviewer_Name  \\\n",
       "0                                                  None  Sunny  Zoeller   \n",
       "1                                                  None   Denise Lesley   \n",
       "10                         {'Color:': ' Black / Black'}       Stephanie   \n",
       "100   {'Color:': ' Rose Gold [6+] CHOOSE CORRECT SIZ...              SG   \n",
       "1000                                               None           Linda   \n",
       "\n",
       "                                                 Review  \\\n",
       "0     Bought it for my husband. He's very happy with it   \n",
       "1     Great screen protector.  Doesn't even seem as ...   \n",
       "10    To tight on my phone and the bottom piece was ...   \n",
       "100   Very good and superior quality, looks great. M...   \n",
       "1000  The charger is not working, however ! The comp...   \n",
       "\n",
       "                           Rev_summ  Review_timestamp Useful Prod_img  \n",
       "0              He's very happy with        1441065600   None     None  \n",
       "1                        Five Stars        1452297600   None     None  \n",
       "10                         One Star        1452729600   None     None  \n",
       "100   Very nice and good quality!!!        1498003200   None     None  \n",
       "1000                      Not happy        1433894400   None     None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating                   0\n",
       "Rev_verify               0\n",
       "Review_Date              0\n",
       "IC                       0\n",
       "Prod_meta           352624\n",
       "Reviewer_Name           91\n",
       "Review                 530\n",
       "Rev_summ               355\n",
       "Review_timestamp         0\n",
       "Useful              698250\n",
       "Prod_img            742256\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for any nulls values\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that are not needed\n",
    "\n",
    "reviews = reviews.drop(['Rev_verify','Review_Date','Prod_meta','Reviewer_Name','Review_timestamp','Useful',\n",
    "                        'Prod_img','Rev_summ'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760450, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>IC</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>B009XD5TPQ</td>\n",
       "      <td>Bought it for my husband. He's very happy with it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>B016MF3P3K</td>\n",
       "      <td>Great screen protector.  Doesn't even seem as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>B00IJJCQBA</td>\n",
       "      <td>To tight on my phone and the bottom piece was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5</td>\n",
       "      <td>B00NIJOGOG</td>\n",
       "      <td>Very good and superior quality, looks great. M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>3</td>\n",
       "      <td>B00MQYS97Y</td>\n",
       "      <td>The charger is not working, however ! The comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating          IC                                             Review\n",
       "0          5  B009XD5TPQ  Bought it for my husband. He's very happy with it\n",
       "1          5  B016MF3P3K  Great screen protector.  Doesn't even seem as ...\n",
       "10         1  B00IJJCQBA  To tight on my phone and the bottom piece was ...\n",
       "100        5  B00NIJOGOG  Very good and superior quality, looks great. M...\n",
       "1000       3  B00MQYS97Y  The charger is not working, however ! The comp..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values\n",
    "reviews = reviews.dropna()\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759920, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting dataset to required format\n",
    "grouped_review= reviews.groupby(['IC'],as_index=True)['Review'].apply(lambda tags: ','.join(tags))\n",
    "grouped_ratings= reviews.groupby(['IC'],as_index=False).agg({'Rating':['min','max','mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        the hirl that wants and needs Hello Kitty prod...\n",
       "1        I was very excited when I first got this case....\n",
       "2        Received this in about a weeks time. Very nice...\n",
       "3        thank you.,Works some of the time.  The charge...\n",
       "4        This Charger is awesome! First of all I wasn't...\n",
       "5        I did not use it for a few months then when I ...\n",
       "6        One of two units didn't work but the company m...\n",
       "7        This charger specifically didn't last that lon...\n",
       "8        Nice case but print started to fade and peel a...\n",
       "9        No label on charger, no indication of output. ...\n",
       "10       QUE COSA TAN MALAAAAA!!!,Overheated & melted a...\n",
       "11       Product charges as described but the plastic q...\n",
       "12       Doesn't fit well. Doesn't let you charge both ...\n",
       "13       I bought 2 of these to go on my children's pho...\n",
       "14       I can go two days and more with this battery. ...\n",
       "15       great item for a really low cost now thats a g...\n",
       "16       Graet...charging quick!,Amazing fast charger,D...\n",
       "17       I wanted to give this charger another chance (...\n",
       "18       Awesome charging case!! I've referred lots of ...\n",
       "19       Recommend to have working well .,Expedite ship...\n",
       "20       Honestly it looks better in the picture.  The ...\n",
       "21       The blue and gray combination with the stylish...\n",
       "22       Looks and works great at a fraction of the pri...\n",
       "23       Excellent,Very good works well.,I read the com...\n",
       "24       good,Works as advertised on both PC and Mac fo...\n",
       "25       I bought this micro usb cable because it was c...\n",
       "26       Palm pre charging data cable works as advertis...\n",
       "27       Love it, recommend.,its a cable, works well, f...\n",
       "28       \"This cable not recommended for this device.\" ...\n",
       "29       This car charger works well and does what it's...\n",
       "                               ...                        \n",
       "48103    Fits perfectly on my s6,The clear part of this...\n",
       "48104    I previous purchased another product by UGreen...\n",
       "48105    So nice not have to worry how I plug in my dev...\n",
       "48106    This is a great Screen protector. My daughter ...\n",
       "48107    These are great to have around the house! You ...\n",
       "48108                                 exactly as described\n",
       "48109    Great product.,I ordered and quickly received ...\n",
       "48110    works great. The clear plastic is durable but ...\n",
       "48111    Excellent quality and work great with the Moto...\n",
       "48112    Sehoo continues to surprise me. Until owning t...\n",
       "48113    This is a waste of money! Do not buy! My touch...\n",
       "48114    Not sure if I like this. It's thicker than my ...\n",
       "48115    Love it I always use tempered glass screen pro...\n",
       "48116    doesn't work if you have a case on your phone,...\n",
       "48117    Product is completely ineffective for transfer...\n",
       "48118    Too heavy and didn't last but about 30 days,co...\n",
       "48119    Look elsewhere, sufferes the same \"streaking p...\n",
       "48120    A great wall charger.  We use it when we trave...\n",
       "48121    cute,Just received the iPhone 6s Case, Clear T...\n",
       "48122    Quite a good fit on my Moto X Pure.  It does a...\n",
       "48123    Best screen protector I have found for this ph...\n",
       "48124    Your device may or may not recognize it as the...\n",
       "48125    I've tried out several different styles of the...\n",
       "48126    Firstly, the packaging of this product is uniq...\n",
       "48127    This is a very well made case. It gives my One...\n",
       "48128    While the case itself does not offer much in t...\n",
       "48129    I've never tried any VR before, but since the ...\n",
       "48130    Nice thick cable, when you plug in to the Kind...\n",
       "48131    Received the cable in two days and it's the be...\n",
       "48132    Just received it and it's VERY thin. No need t...\n",
       "Name: Review, Length: 48133, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp1=grouped_review.reset_index()\n",
    "grp1['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:543: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3108: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IC</th>\n",
       "      <th>Review</th>\n",
       "      <th>(Rating, min)</th>\n",
       "      <th>(Rating, max)</th>\n",
       "      <th>(Rating, mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7508492919</td>\n",
       "      <td>the hirl that wants and needs Hello Kitty prod...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7532385086</td>\n",
       "      <td>I was very excited when I first got this case....</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7887421268</td>\n",
       "      <td>Received this in about a weeks time. Very nice...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8199900164</td>\n",
       "      <td>thank you.,Works some of the time.  The charge...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8288853439</td>\n",
       "      <td>This Charger is awesome! First of all I wasn't...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8288862993</td>\n",
       "      <td>I did not use it for a few months then when I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>828886922X</td>\n",
       "      <td>One of two units didn't work but the company m...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8288878881</td>\n",
       "      <td>This charger specifically didn't last that lon...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9578085451</td>\n",
       "      <td>Nice case but print started to fade and peel a...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>961301375X</td>\n",
       "      <td>No label on charger, no indication of output. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9638762632</td>\n",
       "      <td>QUE COSA TAN MALAAAAA!!!,Overheated &amp; melted a...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9652676748</td>\n",
       "      <td>Product charges as described but the plastic q...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9658231969</td>\n",
       "      <td>Doesn't fit well. Doesn't let you charge both ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9678315173</td>\n",
       "      <td>I bought 2 of these to go on my children's pho...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9707716371</td>\n",
       "      <td>I can go two days and more with this battery. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9707716436</td>\n",
       "      <td>great item for a really low cost now thats a g...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9713957334</td>\n",
       "      <td>Graet...charging quick!,Amazing fast charger,D...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9791151504</td>\n",
       "      <td>I wanted to give this charger another chance (...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9837282037</td>\n",
       "      <td>Awesome charging case!! I've referred lots of ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9838427853</td>\n",
       "      <td>Recommend to have working well .,Expedite ship...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9861247327</td>\n",
       "      <td>Honestly it looks better in the picture.  The ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9861936831</td>\n",
       "      <td>The blue and gray combination with the stylish...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9867853350</td>\n",
       "      <td>Looks and works great at a fraction of the pri...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9966685472</td>\n",
       "      <td>Excellent,Very good works well.,I read the com...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9981710008</td>\n",
       "      <td>good,Works as advertised on both PC and Mac fo...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9981724580</td>\n",
       "      <td>I bought this micro usb cable because it was c...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9983744066</td>\n",
       "      <td>Palm pre charging data cable works as advertis...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9984976505</td>\n",
       "      <td>Love it, recommend.,its a cable, works well, f...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9985039998</td>\n",
       "      <td>\"This cable not recommended for this device.\" ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9985538250</td>\n",
       "      <td>This car charger works well and does what it's...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48103</th>\n",
       "      <td>B01HGIMQ1E</td>\n",
       "      <td>Fits perfectly on my s6,The clear part of this...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48104</th>\n",
       "      <td>B01HGIXL9K</td>\n",
       "      <td>I previous purchased another product by UGreen...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48105</th>\n",
       "      <td>B01HGIXMY4</td>\n",
       "      <td>So nice not have to worry how I plug in my dev...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48106</th>\n",
       "      <td>B01HGSC1UU</td>\n",
       "      <td>This is a great Screen protector. My daughter ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48107</th>\n",
       "      <td>B01HGSOZFY</td>\n",
       "      <td>These are great to have around the house! You ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48108</th>\n",
       "      <td>B01HGTBXSA</td>\n",
       "      <td>exactly as described</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48109</th>\n",
       "      <td>B01HGWYDAC</td>\n",
       "      <td>Great product.,I ordered and quickly received ...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48110</th>\n",
       "      <td>B01HHLMCGO</td>\n",
       "      <td>works great. The clear plastic is durable but ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48111</th>\n",
       "      <td>B01HHLVV2A</td>\n",
       "      <td>Excellent quality and work great with the Moto...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48112</th>\n",
       "      <td>B01HHM0JAE</td>\n",
       "      <td>Sehoo continues to surprise me. Until owning t...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48113</th>\n",
       "      <td>B01HHN56X8</td>\n",
       "      <td>This is a waste of money! Do not buy! My touch...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48114</th>\n",
       "      <td>B01HHNGYR0</td>\n",
       "      <td>Not sure if I like this. It's thicker than my ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48115</th>\n",
       "      <td>B01HHOIUDU</td>\n",
       "      <td>Love it I always use tempered glass screen pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48116</th>\n",
       "      <td>B01HHRFS3C</td>\n",
       "      <td>doesn't work if you have a case on your phone,...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48117</th>\n",
       "      <td>B01HHUWRTM</td>\n",
       "      <td>Product is completely ineffective for transfer...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48118</th>\n",
       "      <td>B01HHVSM7M</td>\n",
       "      <td>Too heavy and didn't last but about 30 days,co...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48119</th>\n",
       "      <td>B01HHVXP56</td>\n",
       "      <td>Look elsewhere, sufferes the same \"streaking p...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48120</th>\n",
       "      <td>B01HI0E8BQ</td>\n",
       "      <td>A great wall charger.  We use it when we trave...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48121</th>\n",
       "      <td>B01HI6X0EQ</td>\n",
       "      <td>cute,Just received the iPhone 6s Case, Clear T...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48122</th>\n",
       "      <td>B01HIDXVC0</td>\n",
       "      <td>Quite a good fit on my Moto X Pure.  It does a...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48123</th>\n",
       "      <td>B01HIDXY2C</td>\n",
       "      <td>Best screen protector I have found for this ph...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48124</th>\n",
       "      <td>B01HIJESIK</td>\n",
       "      <td>Your device may or may not recognize it as the...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48125</th>\n",
       "      <td>B01HIRQLNW</td>\n",
       "      <td>I've tried out several different styles of the...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48126</th>\n",
       "      <td>B01HISNNXC</td>\n",
       "      <td>Firstly, the packaging of this product is uniq...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48127</th>\n",
       "      <td>B01HITC2O2</td>\n",
       "      <td>This is a very well made case. It gives my One...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48128</th>\n",
       "      <td>B01HJBS5C2</td>\n",
       "      <td>While the case itself does not offer much in t...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48129</th>\n",
       "      <td>B01HJC7N4C</td>\n",
       "      <td>I've never tried any VR before, but since the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48130</th>\n",
       "      <td>B01HJCN1UC</td>\n",
       "      <td>Nice thick cable, when you plug in to the Kind...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48131</th>\n",
       "      <td>B01HJCN55I</td>\n",
       "      <td>Received the cable in two days and it's the be...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48132</th>\n",
       "      <td>B01HJH9IN6</td>\n",
       "      <td>Just received it and it's VERY thin. No need t...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48133 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               IC                                             Review  \\\n",
       "0      7508492919  the hirl that wants and needs Hello Kitty prod...   \n",
       "1      7532385086  I was very excited when I first got this case....   \n",
       "2      7887421268  Received this in about a weeks time. Very nice...   \n",
       "3      8199900164  thank you.,Works some of the time.  The charge...   \n",
       "4      8288853439  This Charger is awesome! First of all I wasn't...   \n",
       "5      8288862993  I did not use it for a few months then when I ...   \n",
       "6      828886922X  One of two units didn't work but the company m...   \n",
       "7      8288878881  This charger specifically didn't last that lon...   \n",
       "8      9578085451  Nice case but print started to fade and peel a...   \n",
       "9      961301375X  No label on charger, no indication of output. ...   \n",
       "10     9638762632  QUE COSA TAN MALAAAAA!!!,Overheated & melted a...   \n",
       "11     9652676748  Product charges as described but the plastic q...   \n",
       "12     9658231969  Doesn't fit well. Doesn't let you charge both ...   \n",
       "13     9678315173  I bought 2 of these to go on my children's pho...   \n",
       "14     9707716371  I can go two days and more with this battery. ...   \n",
       "15     9707716436  great item for a really low cost now thats a g...   \n",
       "16     9713957334  Graet...charging quick!,Amazing fast charger,D...   \n",
       "17     9791151504  I wanted to give this charger another chance (...   \n",
       "18     9837282037  Awesome charging case!! I've referred lots of ...   \n",
       "19     9838427853  Recommend to have working well .,Expedite ship...   \n",
       "20     9861247327  Honestly it looks better in the picture.  The ...   \n",
       "21     9861936831  The blue and gray combination with the stylish...   \n",
       "22     9867853350  Looks and works great at a fraction of the pri...   \n",
       "23     9966685472  Excellent,Very good works well.,I read the com...   \n",
       "24     9981710008  good,Works as advertised on both PC and Mac fo...   \n",
       "25     9981724580  I bought this micro usb cable because it was c...   \n",
       "26     9983744066  Palm pre charging data cable works as advertis...   \n",
       "27     9984976505  Love it, recommend.,its a cable, works well, f...   \n",
       "28     9985039998  \"This cable not recommended for this device.\" ...   \n",
       "29     9985538250  This car charger works well and does what it's...   \n",
       "...           ...                                                ...   \n",
       "48103  B01HGIMQ1E  Fits perfectly on my s6,The clear part of this...   \n",
       "48104  B01HGIXL9K  I previous purchased another product by UGreen...   \n",
       "48105  B01HGIXMY4  So nice not have to worry how I plug in my dev...   \n",
       "48106  B01HGSC1UU  This is a great Screen protector. My daughter ...   \n",
       "48107  B01HGSOZFY  These are great to have around the house! You ...   \n",
       "48108  B01HGTBXSA                               exactly as described   \n",
       "48109  B01HGWYDAC  Great product.,I ordered and quickly received ...   \n",
       "48110  B01HHLMCGO  works great. The clear plastic is durable but ...   \n",
       "48111  B01HHLVV2A  Excellent quality and work great with the Moto...   \n",
       "48112  B01HHM0JAE  Sehoo continues to surprise me. Until owning t...   \n",
       "48113  B01HHN56X8  This is a waste of money! Do not buy! My touch...   \n",
       "48114  B01HHNGYR0  Not sure if I like this. It's thicker than my ...   \n",
       "48115  B01HHOIUDU  Love it I always use tempered glass screen pro...   \n",
       "48116  B01HHRFS3C  doesn't work if you have a case on your phone,...   \n",
       "48117  B01HHUWRTM  Product is completely ineffective for transfer...   \n",
       "48118  B01HHVSM7M  Too heavy and didn't last but about 30 days,co...   \n",
       "48119  B01HHVXP56  Look elsewhere, sufferes the same \"streaking p...   \n",
       "48120  B01HI0E8BQ  A great wall charger.  We use it when we trave...   \n",
       "48121  B01HI6X0EQ  cute,Just received the iPhone 6s Case, Clear T...   \n",
       "48122  B01HIDXVC0  Quite a good fit on my Moto X Pure.  It does a...   \n",
       "48123  B01HIDXY2C  Best screen protector I have found for this ph...   \n",
       "48124  B01HIJESIK  Your device may or may not recognize it as the...   \n",
       "48125  B01HIRQLNW  I've tried out several different styles of the...   \n",
       "48126  B01HISNNXC  Firstly, the packaging of this product is uniq...   \n",
       "48127  B01HITC2O2  This is a very well made case. It gives my One...   \n",
       "48128  B01HJBS5C2  While the case itself does not offer much in t...   \n",
       "48129  B01HJC7N4C  I've never tried any VR before, but since the ...   \n",
       "48130  B01HJCN1UC  Nice thick cable, when you plug in to the Kind...   \n",
       "48131  B01HJCN55I  Received the cable in two days and it's the be...   \n",
       "48132  B01HJH9IN6  Just received it and it's VERY thin. No need t...   \n",
       "\n",
       "       (Rating, min)  (Rating, max)  (Rating, mean)  \n",
       "0                  1              5        4.300000  \n",
       "1                  1              5        3.285714  \n",
       "2                  1              5        3.538462  \n",
       "3                  2              5        3.750000  \n",
       "4                  1              5        3.818182  \n",
       "5                  1              5        3.656250  \n",
       "6                  3              5        4.333333  \n",
       "7                  1              5        3.977273  \n",
       "8                  3              5        4.250000  \n",
       "9                  1              5        3.923077  \n",
       "10                 1              5        2.818182  \n",
       "11                 2              5        3.600000  \n",
       "12                 1              5        3.333333  \n",
       "13                 4              5        4.714286  \n",
       "14                 1              5        4.109375  \n",
       "15                 4              5        4.600000  \n",
       "16                 2              5        4.250000  \n",
       "17                 1              5        3.250000  \n",
       "18                 3              5        4.428571  \n",
       "19                 1              5        4.142857  \n",
       "20                 3              5        4.500000  \n",
       "21                 3              5        4.555556  \n",
       "22                 3              5        4.571429  \n",
       "23                 1              5        4.166667  \n",
       "24                 5              5        5.000000  \n",
       "25                 3              5        4.200000  \n",
       "26                 1              5        3.800000  \n",
       "27                 5              5        5.000000  \n",
       "28                 1              5        3.400000  \n",
       "29                 1              5        4.000000  \n",
       "...              ...            ...             ...  \n",
       "48103              1              5        3.555556  \n",
       "48104              1              5        2.666667  \n",
       "48105              1              5        3.777778  \n",
       "48106              5              5        5.000000  \n",
       "48107              5              5        5.000000  \n",
       "48108              5              5        5.000000  \n",
       "48109              2              5        3.800000  \n",
       "48110              4              4        4.000000  \n",
       "48111              5              5        5.000000  \n",
       "48112              1              5        4.000000  \n",
       "48113              1              5        2.142857  \n",
       "48114              3              5        4.285714  \n",
       "48115              1              5        4.000000  \n",
       "48116              2              5        3.000000  \n",
       "48117              1              5        4.200000  \n",
       "48118              1              5        3.571429  \n",
       "48119              1              5        3.000000  \n",
       "48120              3              5        4.733333  \n",
       "48121              5              5        5.000000  \n",
       "48122              5              5        5.000000  \n",
       "48123              2              5        4.000000  \n",
       "48124              1              5        3.857143  \n",
       "48125              3              5        4.750000  \n",
       "48126              1              5        3.500000  \n",
       "48127              3              5        4.600000  \n",
       "48128              1              5        3.250000  \n",
       "48129              3              5        4.600000  \n",
       "48130              4              5        4.727273  \n",
       "48131              2              5        4.454545  \n",
       "48132              1              5        2.833333  \n",
       "\n",
       "[48133 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=pd.merge(grp1,grouped_ratings,on='IC')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 1\n",
      "the hirl that wants and needs Hello Kitty products\n",
      "Was so happy that she has a collection to choose from\n",
      "all her friends wants hello kitty phone cases to\n",
      "thanks seller,so cute love this...one drop and its done though...i dropped iton the bathroom floor and it cracked. other then that item is very cute,this case is so cute it looks good on my white iphone its pretty good quality only one diamond has fallen off and its cos i dropped it on the cement. its kinda hard to get off but whatever its so cute and hot and everyone compliments me on it,Happy it,This case is very pretty.  It is very girly looking.  The only problem is it will be hard to put the case in a jeans pocket because the bow does stick out.  It would not be a problem for me as I carry mine in my purse, but my daughters put their phones in their pockets.  Shipping was as described.,Cute case. Jewels do come off but what do you expect. Mine has held up pretty nicely and lost a few stones but is barely noticeable.,Got this for a friend who saw this and liked it. i got it for her, and she liked it. looks good on your iphone and fits well too,It is such a good case for a low price. I have it on right now and never had any problems with it besides losing about 2 stones since my purchase because I'm rough with it. I recommend it.,I didn't get the same case that's shown in the picture. The case I got had a black and gold bow. Its still cute but not what I wanted. I wanted what was in the picture.,it is a beautiful phone case but its also hard to remove & also it the 3d ribbon came wasy darker than the picture shows & it also came wonky but its pretty,It looks like I actually bedazzled my phone.  The case is better than what I expected ... it is so cute.  Yes, I would recommend this case.,My teenage daughter LOVED this case.  It is really pretty.\n",
      "\n",
      "I only put 4 stars because it does look a little cheap looking, but what do you expect with this much bling. ;o),this is the most cutest case i've ever bought from here , i just got it today .. it took a really long time,which is weird cause it took almost two months. but i didn't care. i thought that it was gonna fall apart. but it didn't it really held up nicely. & i am thinking about ordering the other ones that come in pink & black. these are really classy & rich looking for only 2 bucks you can not go wrong. its so girly,even though i don't like too much stuff going on,on my cases. this i perfect, i haven't tried fitting it in my pocket but i will  sooo. you should definatley buy this.,It is so cute!  Love the bling and love the pink!  It is such a cute case and I get many compliments.,This case is soo super pretty! This case may look very breakable but it is very sturdy. It is super cute, the case looks cuter in real life than the picture. SO WHAT ARE YOU WAITING FOR?? GET IT!!!,For the price i received a pretty great case! I received many compliments on how cute it was! the bow on the back looked like it might fall out but i have had it for 6 months and nope, its strongly attached! the diamonds in the back amazed me how long they stayed attached and only like 3 have fallen off, not too bad. it fits nicely on my phone. but i don't recommend dropping your phone because it doesn't give you protection from impacts! its more like a \"look at my pretty phone\" kind of case. Unfortunately i had to replace my case because due to my crazy lifestyle (throwing my phone in bags and pockets) the plastic by the volume buttons started to break off. But i will definitely buy another one to put on my iPhone on special occasions only!,love it!!!!!\n",
      "its gorgeous\n",
      "arrived on time i get many compliments on it\n",
      "its definitely a girly case which i love,Beautiful quality and outstanding product! Everyone compliments me on the case and thinks I spent wayy wayy more than I really did :),I love this case. I have plenty of cases but this is 1 of my favs.. I kept this case on longer than any case I have. Sum of the stones fall off but they give u extra. Once I ran out of extras I changed da case.... Lol,Another great product that my daughter she use it for a long time but now she has the iPhone 5c. Transaction went well and fast.,This case was an extremely great deal. The color and design was perfect. This is a great going out on the town case.,crystals fell off as nothing :( that's why I really didn't like it but as soon as I saw it I liked it but the stones,Can not argue with the price or appearance. Looks just like the picture with the exception that it is not white. The jewels do fall off rather easily.,soooo cute! I got many complaints when I had this cover.  I took the gems off at the bottom of the case since texting made it hard.  After awhile, the gems started to peel off..but it still looked cute!,I have used this case for a couple weeks & so far its been great!  All the little blings have stayed in place.  Very sparkly & cute.,The product is recommended for anyone looking to use it or gift it to one of your family members. Thanks,here is another I love as well!!!, I am so happy with my phone cover,, I get so many compliments from other females!!!! but it is my phone that wears it!!!!,I used this case for not even a week and the bow came off. I loved it so pretty, but wish it would of stayed together.,I really love this case... you have to keep your phone face down all the time, but it's pretty good quality. Some stones come off from normal use, unless you keep it in your purse all day.,gotten so many compliments, its so beautiful and simple. i've had it for 3 months now and it still looks brand new,It was a great deal,, I would keep buying !!!!!!!\n",
      "It was worth the wait\n",
      "Happy happy happy happy happy,Very cheap broke the first time we put it on :( It was pretty but very cheaply made!! Too bad as it is cute,so the case came on time, i love the design. I'm actually missing 2 studs but nothing too noticeable the studding is almost a bit sloppy around the bow, but once again not too noticeable. I haven't put in my phone yet so this is just what I've notice so far,THIS IS A VERY PRETTY 4S CELL PHONE CASE ANS IT IS 3D AND VERY PRETTY, THANK YOU FROM RENE,I ordered this case for my daughter for Christmas because she got an IPhone for Christmas and I wanted her to have a really nice phone case that was different because EVERYBODY has the same IPhone cases. SHE LOVED THIS PHONE CASE FROM THE MOMENT SHE OPENED THE PACKAGE!! I loved the fact that this case came a few days earlier than expected and THE PRICE WAS AMAZING for a really cute 3D IPhone case too.,I purchased this for my grand-daughters phone. She loves it and it is so unique. The quality of the cover is very good.,really pretty! ive had a couple gems come off, but it comes with glue, so its easily fixable. i like it,When you don't want to spend a whole lot of cash but want a great deal...this is the shop to buy from!,I liked it because it was cute, but the studs fall off easily and to protect a phone this would not be recommended. Buy if you just like it for looks.,It's very cute and the stones lasted a long time!  It really makes your phone look a lot more prettier and funnier.\n",
      "\n",
      "Review # 2\n",
      "I was very excited when I first got this case. I loved the color and the feel of it. Then I started noticing stuff on my hands and realized the \"rubber\" peels right off the phone just from normal use. As the saying goes you get what you pay for.,Received this item very quickly. The design is even more vivid than expected. The cover is soft (rubberized) but durable. I have received many compliments. It was an excellent buy! I would recommend this to anyone wanting a \"good look\" for their phone.,The case is not white, its more of a silver. It's still a very pretty case- it fit my phone perfectly. It took too long to arrive.,I didn't like this,good,I wanted to highly recommend this seller>I had a slight issue with the product, and they offered a complete refund without having to send the product back:) I fixed the case, so there was no need for the refund. Thanks for being an honest company!!!!!!,this cover makes an old phone look and feel new. I like that I can order covers for little money and snazzy up my phone.\n",
      "\n",
      "Review # 3\n",
      "Received this in about a weeks time. Very nice actually considering how cheap the price was. It's a pink/purple with black leopard spots and has a very nice feel to it. I haven't tried to remove it yet, but hopefully it won't give me any trouble when I do want to change covers.,Good product, good price, fast shipping,I loved the case when I first received it but shortly after, the case started to peel off at first i did not know what it was until i looked on the back of the case and it was missing spots. Guess sometimes a good deal is not really a good deal.would not purchase.,this is a great product, product shipped extreamly fast, no problems or defects with product, recommended buy for anyone. A+,my daughter liked it for a few days and then i didn't see it anymore on the phone. Use your judgement on that one.,Very nice case and color too, only that the material is weak and soon broke.\n",
      "\n",
      "Thank you very much,This case is afordable and yes it peels very easy but thats to be expected because its a dollor. I bought mine from Electromaster. It snaps on fine for me but if I were to drop my phone the phone would be fine but the case would surely be done for. its only a dollar so do not get your hopes up. it looks great but its for show not for protection. if you want your phone to be protected go the extra mile for an outter box,Sent case that didn't fit,phone case does not work. the clips may be broken, it does not work. the case is a little scratch and missing paint. it fits my phone but does not stay clipped!,Didn't fit my phone.,The color is more dark in person, and this isn't the best protection case. I get a lot of compliments on the case, and it was easy to put on.,Simple to install and a great way to help protect your phone.\n",
      "My wife really liked the ability to change the look of her phone.,Good item\n",
      "\n",
      "Review # 4\n",
      "thank you.,Works some of the time.  The charger had stopped working for a few days but seeems to be okay now.  Seems to have a mind of its own.  Lol.....,bought this for my phone and it didn't work. the phone beeps on and off and doesnt work when charging.,Awesome!\n",
      "\n",
      "Review # 5\n",
      "This Charger is awesome! First of all I wasn't expecting it until between 12/12  and 12/17, but it arrived on 12/11,So That was a plus for me! Second, I was kinda skeptical about purchasing it because it says that its for a Galaxy S and Blackberry so that can mean a lot of things! but I figured what the heck its cheap, so what can I loose!  Well, I just got it today and decided to give it a try at work on my Galaxy S4,  it charged  from 80%  to 100%  in about 7 minutes!! Yes i timed it!  and also the cord is long which is a plus for me!  So if your looking for a fast charger with a long cord and not to mention a great price, this is the one!!\n",
      "\n",
      "update:  not sure what the other reviewer meant by saying these are poor quality chargers, because I've had mine for a little over 2 weeks and it's still going strong!!  Love this charger  especially  for the length!!,You are saying that you sell a Universal charger but what you sold me was a blasted Blackberry charger! I am so mad right now!! I can't believe this. I was so excited waiting for a charger for my samsung battery charger and all I got was a BLACKBERRY CHARGER. DON'T EVER BUY FROM THIS SELLER.,Good,what can i say? pretty much like all the travel chargers for my galaxy 2 phone. price was the best,I would not recommend getting this charger. I can literally have my phone (samsung galaxy 2) charging for over 6 hours and it will not be fully charged. This is the only charger I have ever bought that sucks like this. Do not buy,Works as advertized...,This charger woks wonderfully on the Samsung Galaxy S4.  it chargers the phone fast and the price is great!  Highly Recommended product!,so far no problems, it charges fast that is what i need it. it doesn' have an USB as it specified, jus a direct wall charger.,Shipping on time.  We'll packaged.  Charges like a champ! Definitely will rapid charge the Galaxy 3 with no problems.  Definite must have with a phone happy teenager,Didn't work,Great ! So far.\n",
      "I had this charger before and it seem to last pretty good 3 years fits most phones except Iphone and Galaxy 5  but that phone can still use it in the right side of the portal it will Judy charge it a bit slower. When this charger goes it tends to loose it's fit firmly in usb and has a short.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting some of the reviews\n",
    "for i in range(5):\n",
    "    print(\"Review #\",i+1)\n",
    "    print(result.Review[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        # We are not using \"text.split()\" here\n",
    "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
    "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great piece art believe may'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"That's a great piece of art,Can you believe it?I've.But you may not.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the summaries and texts\n",
    "\n",
    "We will remove the stopwords from the texts because they do not provide much use for training our model. However, we will keep them for our summaries so that they sound more like natural phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_texts = []\n",
    "for text in result.Review:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Reviews are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "hirl wants needs hello kitty products happy collection choose friends wants hello kitty phone cases thanks seller cute love one drop done though dropped iton bathroom floor cracked item cute case cute looks good white iphone pretty good quality one diamond fallen cos dropped cement kinda hard get whatever cute hot everyone compliments happy case pretty girly looking problem hard put case jeans pocket bow stick would problem carry mine purse daughters put phones pockets shipping described cute case jewels come expect mine held pretty nicely lost stones barely noticeable got friend saw liked got liked looks good iphone fits well good case low price right never problems besides losing 2 stones since purchase rough recommend get case shown picture case got black gold bow still cute wanted wanted picture beautiful phone case also hard remove also 3d ribbon came wasy darker picture shows also came wonky pretty looks like actually bedazzled phone case better expected cute yes would recommend case teenage daughter loved case really pretty put 4 stars look little cheap looking expect much bling cutest case ever bought got today took really long time weird cause took almost two months care thought gonna fall apart really held nicely thinking ordering ones come pink black really classy rich looking 2 bucks go wrong girly even though like much stuff going cases perfect tried fitting pocket sooo definatley buy cute love bling love pink cute case get many compliments case soo super pretty case may look breakable sturdy super cute case looks cuter real life picture waiting get price received pretty great case received many compliments cute bow back looked like might fall 6 months nope strongly attached diamonds back amazed long stayed attached like 3 fallen bad fits nicely phone recommend dropping phone give protection impacts like look pretty phone kind case unfortunately replace case due crazy lifestyle throwing phone bags pockets plastic volume buttons started break definitely buy another one put iphone special occasions love gorgeous arrived time get many compliments definitely girly case love beautiful quality outstanding product everyone compliments case thinks spent wayy wayy really love case plenty cases 1 favs kept case longer case sum stones fall give u extra ran extras changed da case lol another great product daughter use long time iphone 5c transaction went well fast case extremely great deal color design perfect great going town case crystals fell nothing really like soon saw liked stones argue price appearance looks like picture exception white jewels fall rather easily soooo cute got many complaints cover took gems bottom case since texting made hard awhile gems started peel still looked cute used case couple weeks far great little blings stayed place sparkly cute product recommended anyone looking use gift one family members thanks another love well happy phone cover get many compliments females phone wears used case even week bow came loved pretty wish would stayed together really love case keep phone face time pretty good quality stones come normal use unless keep purse day gotten many compliments beautiful simple 3 months still looks brand new great deal would keep buying worth wait happy happy happy happy happy cheap broke first time put pretty cheaply made bad cute case came time love design actually missing 2 studs nothing noticeable studding almost bit sloppy around bow noticeable put phone yet notice far pretty 4s cell phone case ans 3d pretty thank rene ordered case daughter christmas got iphone christmas wanted really nice phone case different everybody iphone cases loved phone case moment opened package loved fact case came days earlier expected price amazing really cute 3d iphone case purchased grand daughters phone loves unique quality cover good really pretty ive couple gems come comes glue easily fixable like want spend whole lot cash want great deal shop buy liked cute studs fall easily protect phone would recommended buy like looks cute stones lasted long time really makes phone look lot prettier funnier\n",
      "\n",
      "Clean Review # 2\n",
      "excited first got case loved color feel started noticing stuff hands realized rubber peels right phone normal use saying goes get pay received item quickly design even vivid expected cover soft rubberized durable received many compliments excellent buy would recommend anyone wanting good look phone case white silver still pretty case fit phone perfectly took long arrive like good wanted highly recommend seller slight issue product offered complete refund without send product back fixed case need refund thanks honest company cover makes old phone look feel new like order covers little money snazzy phone\n",
      "\n",
      "Clean Review # 3\n",
      "received weeks time nice actually considering cheap price pink purple black leopard spots nice feel tried remove yet hopefully give trouble want change covers good product good price fast shipping loved case first received shortly case started peel first know looked back case missing spots guess sometimes good deal really good deal would purchase great product product shipped extreamly fast problems defects product recommended buy anyone daughter liked days see anymore phone use judgement one nice case color material weak soon broke thank much case afordable yes peels easy thats expected dollor bought mine electromaster snaps fine drop phone phone would fine case would surely done dollar get hopes looks great show protection want phone protected go extra mile outter box sent case fit phone case work clips may broken work case little scratch missing paint fits phone stay clipped fit phone color dark person best protection case get lot compliments case easy put simple install great way help protect phone wife really liked ability change look phone good item\n",
      "\n",
      "Clean Review # 4\n",
      "thank works time charger stopped working days seeems okay seems mind lol bought phone work phone beeps doesnt work charging awesome\n",
      "\n",
      "Clean Review # 5\n",
      "charger awesome first expecting 12 12 12 17 arrived 12 11 plus second kinda skeptical purchasing says galaxy blackberry mean lot things figured heck cheap loose well got today decided give try work galaxy s4 charged 80 100 7 minutes yes timed also cord long plus looking fast charger long cord mention great price one update sure reviewer meant saying poor quality chargers mine little 2 weeks still going strong love charger especially length saying sell universal charger sold blasted blackberry charger mad right cannot believe excited waiting charger samsung battery charger got blackberry charger ever buy seller good say pretty much like travel chargers galaxy 2 phone price best would recommend getting charger literally phone samsung galaxy 2 charging 6 hours fully charged charger ever bought sucks like buy works advertized charger woks wonderfully samsung galaxy s4 chargers phone fast price great highly recommended product far problems charges fast need usb specified jus direct wall charger shipping time packaged charges like champ definitely rapid charge galaxy 3 problems definite must phone happy teenager work great far charger seem last pretty good 3 years fits phones except iphone galaxy 5 phone still use right side portal judy charge bit slower charger goes tends loose fit firmly usb short\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(5):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of occurrences of each word in a set of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the function a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 1, 'is': 1, 'a': 2, 'great': 4, 'leader': 2, 'India': 1, 'has': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {}\n",
    "count_words(mydict, [\"that is a great great great leader\",\"India has a great leader\"])\n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 107606\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, clean_texts)\n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38745"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how many times \"glass\" occurs in the data\n",
    "word_counts[\"glass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better\n",
    "(https://github.com/commonsense/conceptnet-numberbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417195\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('numberbatch-en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the CN embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"glass\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "\n",
    "I use a threshold of 20, so that words not in CN can be added to our word_embedding_matrix, but they need to be common enough in the reviews so that the model can understand their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 1948\n",
      "Percent of words that are missing from vocabulary: 1.81%\n"
     ]
    }
   ],
   "source": [
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those missing words in the CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outter', 193),\n",
       " ('doesnt', 4329),\n",
       " ('12', 4432),\n",
       " ('17', 1049),\n",
       " ('11', 2088),\n",
       " ('80', 1386),\n",
       " ('100', 10990),\n",
       " ('advertized', 126),\n",
       " ('woks', 39),\n",
       " ('waze', 252),\n",
       " ('sii', 262),\n",
       " ('microusb', 1145),\n",
       " ('excelente', 2964),\n",
       " ('calidad', 89),\n",
       " ('ningun', 21),\n",
       " ('gs2', 76),\n",
       " ('2013', 1303),\n",
       " ('chager', 29),\n",
       " ('13', 1675),\n",
       " ('samsungs', 188),\n",
       " ('sansung', 22),\n",
       " ('i9100', 39),\n",
       " ('htc', 8176),\n",
       " ('note3', 230),\n",
       " ('50', 4205),\n",
       " ('75', 1400),\n",
       " ('90', 1762),\n",
       " ('40', 2410),\n",
       " ('70', 1083),\n",
       " ('powerbear', 88)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_words = []\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold and word not in embeddings_index:\n",
    "        missing_words.append((word,count))\n",
    "missing_words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks mostly products' brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words to indexes, indexes to words dicts\n",
    "\n",
    "Limit the vocab that we will use to words that appear ≥ threshold or are in CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 107606\n",
      "Number of words we will use: 44966\n",
      "Percent of words we will use: 41.79%\n"
     ]
    }
   ],
   "source": [
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "# Index words from 0\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word embedding matrix\n",
    "It has shape (nb_words, embedding_dim) i.e. (44966, 300) in this case. 1st dim is word index, 2nd dim is from CN or random generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44966\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Function to convert sentences to sequence of words indexes\n",
    "\n",
    "It also use <UNK> index to replace unknown words, append <EOS> (End of Sentence) to the sequences if eos is set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply convert_to_ints to clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in clean_text: 19681670\n",
      "Total number of UNKs in clean_text: 123301\n",
      "Percent of words that are UNK: 0.63%\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in clean_text:\", word_count)\n",
    "print(\"Total number of UNKs in clean_text:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at what the sequence looks like\n",
    "\n",
    "Each number here represents a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[44962,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  2,\n",
       "  3,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  44962,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  13,\n",
       "  24,\n",
       "  13,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  26,\n",
       "  30,\n",
       "  15,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  19,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  13,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  5,\n",
       "  24,\n",
       "  29,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  36,\n",
       "  45,\n",
       "  24,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  44,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  45,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  13,\n",
       "  24,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  52,\n",
       "  62,\n",
       "  29,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  68,\n",
       "  71,\n",
       "  25,\n",
       "  26,\n",
       "  28,\n",
       "  72,\n",
       "  73,\n",
       "  26,\n",
       "  24,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  65,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  37,\n",
       "  24,\n",
       "  86,\n",
       "  87,\n",
       "  24,\n",
       "  68,\n",
       "  88,\n",
       "  89,\n",
       "  48,\n",
       "  90,\n",
       "  13,\n",
       "  91,\n",
       "  91,\n",
       "  87,\n",
       "  92,\n",
       "  9,\n",
       "  24,\n",
       "  93,\n",
       "  36,\n",
       "  94,\n",
       "  93,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  44962,\n",
       "  98,\n",
       "  87,\n",
       "  99,\n",
       "  93,\n",
       "  97,\n",
       "  100,\n",
       "  29,\n",
       "  25,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  9,\n",
       "  24,\n",
       "  104,\n",
       "  105,\n",
       "  13,\n",
       "  106,\n",
       "  50,\n",
       "  85,\n",
       "  24,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  24,\n",
       "  110,\n",
       "  29,\n",
       "  45,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  43,\n",
       "  61,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  24,\n",
       "  119,\n",
       "  120,\n",
       "  68,\n",
       "  121,\n",
       "  122,\n",
       "  110,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  122,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  110,\n",
       "  62,\n",
       "  63,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  60,\n",
       "  138,\n",
       "  88,\n",
       "  110,\n",
       "  139,\n",
       "  140,\n",
       "  43,\n",
       "  81,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  42,\n",
       "  144,\n",
       "  18,\n",
       "  101,\n",
       "  116,\n",
       "  145,\n",
       "  146,\n",
       "  10,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  47,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  13,\n",
       "  14,\n",
       "  117,\n",
       "  14,\n",
       "  138,\n",
       "  13,\n",
       "  24,\n",
       "  37,\n",
       "  153,\n",
       "  41,\n",
       "  24,\n",
       "  154,\n",
       "  155,\n",
       "  29,\n",
       "  24,\n",
       "  156,\n",
       "  113,\n",
       "  157,\n",
       "  158,\n",
       "  155,\n",
       "  13,\n",
       "  24,\n",
       "  25,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  87,\n",
       "  162,\n",
       "  37,\n",
       "  75,\n",
       "  163,\n",
       "  29,\n",
       "  164,\n",
       "  24,\n",
       "  163,\n",
       "  153,\n",
       "  41,\n",
       "  13,\n",
       "  48,\n",
       "  165,\n",
       "  166,\n",
       "  101,\n",
       "  167,\n",
       "  133,\n",
       "  168,\n",
       "  129,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  165,\n",
       "  173,\n",
       "  123,\n",
       "  174,\n",
       "  171,\n",
       "  101,\n",
       "  175,\n",
       "  32,\n",
       "  176,\n",
       "  72,\n",
       "  63,\n",
       "  9,\n",
       "  85,\n",
       "  177,\n",
       "  9,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  101,\n",
       "  113,\n",
       "  29,\n",
       "  9,\n",
       "  181,\n",
       "  24,\n",
       "  182,\n",
       "  183,\n",
       "  24,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  9,\n",
       "  188,\n",
       "  56,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  152,\n",
       "  195,\n",
       "  15,\n",
       "  45,\n",
       "  28,\n",
       "  196,\n",
       "  197,\n",
       "  14,\n",
       "  198,\n",
       "  199,\n",
       "  124,\n",
       "  37,\n",
       "  153,\n",
       "  41,\n",
       "  194,\n",
       "  42,\n",
       "  24,\n",
       "  14,\n",
       "  92,\n",
       "  30,\n",
       "  200,\n",
       "  201,\n",
       "  40,\n",
       "  41,\n",
       "  24,\n",
       "  202,\n",
       "  203,\n",
       "  44962,\n",
       "  44962,\n",
       "  110,\n",
       "  14,\n",
       "  24,\n",
       "  204,\n",
       "  10,\n",
       "  205,\n",
       "  44962,\n",
       "  206,\n",
       "  24,\n",
       "  207,\n",
       "  24,\n",
       "  208,\n",
       "  65,\n",
       "  133,\n",
       "  178,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  24,\n",
       "  215,\n",
       "  195,\n",
       "  164,\n",
       "  201,\n",
       "  108,\n",
       "  216,\n",
       "  123,\n",
       "  124,\n",
       "  28,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  73,\n",
       "  220,\n",
       "  24,\n",
       "  221,\n",
       "  164,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  147,\n",
       "  164,\n",
       "  146,\n",
       "  225,\n",
       "  24,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  110,\n",
       "  101,\n",
       "  229,\n",
       "  70,\n",
       "  71,\n",
       "  65,\n",
       "  230,\n",
       "  75,\n",
       "  231,\n",
       "  25,\n",
       "  101,\n",
       "  87,\n",
       "  232,\n",
       "  27,\n",
       "  59,\n",
       "  133,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  13,\n",
       "  68,\n",
       "  153,\n",
       "  236,\n",
       "  237,\n",
       "  122,\n",
       "  238,\n",
       "  239,\n",
       "  24,\n",
       "  82,\n",
       "  240,\n",
       "  241,\n",
       "  36,\n",
       "  242,\n",
       "  238,\n",
       "  192,\n",
       "  243,\n",
       "  90,\n",
       "  166,\n",
       "  13,\n",
       "  244,\n",
       "  24,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  164,\n",
       "  114,\n",
       "  44962,\n",
       "  174,\n",
       "  248,\n",
       "  249,\n",
       "  13,\n",
       "  201,\n",
       "  250,\n",
       "  251,\n",
       "  43,\n",
       "  216,\n",
       "  252,\n",
       "  15,\n",
       "  253,\n",
       "  254,\n",
       "  11,\n",
       "  195,\n",
       "  14,\n",
       "  73,\n",
       "  5,\n",
       "  9,\n",
       "  237,\n",
       "  37,\n",
       "  153,\n",
       "  41,\n",
       "  255,\n",
       "  9,\n",
       "  256,\n",
       "  244,\n",
       "  24,\n",
       "  144,\n",
       "  257,\n",
       "  48,\n",
       "  97,\n",
       "  109,\n",
       "  29,\n",
       "  258,\n",
       "  50,\n",
       "  174,\n",
       "  259,\n",
       "  110,\n",
       "  14,\n",
       "  24,\n",
       "  260,\n",
       "  9,\n",
       "  261,\n",
       "  124,\n",
       "  29,\n",
       "  26,\n",
       "  30,\n",
       "  65,\n",
       "  60,\n",
       "  262,\n",
       "  216,\n",
       "  263,\n",
       "  260,\n",
       "  53,\n",
       "  264,\n",
       "  265,\n",
       "  153,\n",
       "  41,\n",
       "  92,\n",
       "  266,\n",
       "  175,\n",
       "  129,\n",
       "  90,\n",
       "  25,\n",
       "  267,\n",
       "  268,\n",
       "  164,\n",
       "  222,\n",
       "  50,\n",
       "  260,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  115,\n",
       "  272,\n",
       "  273,\n",
       "  124,\n",
       "  45,\n",
       "  29,\n",
       "  274,\n",
       "  241,\n",
       "  176,\n",
       "  13,\n",
       "  24,\n",
       "  97,\n",
       "  124,\n",
       "  14,\n",
       "  224,\n",
       "  102,\n",
       "  275,\n",
       "  81,\n",
       "  276,\n",
       "  228,\n",
       "  67,\n",
       "  277,\n",
       "  127,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  48,\n",
       "  67,\n",
       "  45,\n",
       "  9,\n",
       "  281,\n",
       "  282,\n",
       "  247,\n",
       "  29,\n",
       "  283,\n",
       "  284,\n",
       "  9,\n",
       "  24,\n",
       "  285,\n",
       "  95,\n",
       "  29,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  24,\n",
       "  108,\n",
       "  289,\n",
       "  68,\n",
       "  28,\n",
       "  289,\n",
       "  91,\n",
       "  110,\n",
       "  290,\n",
       "  9,\n",
       "  24,\n",
       "  291,\n",
       "  292,\n",
       "  28,\n",
       "  10,\n",
       "  109,\n",
       "  9,\n",
       "  24,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  109,\n",
       "  296,\n",
       "  24,\n",
       "  97,\n",
       "  297,\n",
       "  298,\n",
       "  105,\n",
       "  75,\n",
       "  299,\n",
       "  110,\n",
       "  13,\n",
       "  95,\n",
       "  28,\n",
       "  24,\n",
       "  300,\n",
       "  301,\n",
       "  54,\n",
       "  9,\n",
       "  302,\n",
       "  303,\n",
       "  30,\n",
       "  237,\n",
       "  26,\n",
       "  110,\n",
       "  29,\n",
       "  304,\n",
       "  245,\n",
       "  238,\n",
       "  60,\n",
       "  305,\n",
       "  306,\n",
       "  234,\n",
       "  307,\n",
       "  101,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  308,\n",
       "  164,\n",
       "  222,\n",
       "  313,\n",
       "  152,\n",
       "  71,\n",
       "  13,\n",
       "  276,\n",
       "  133,\n",
       "  234,\n",
       "  314,\n",
       "  9,\n",
       "  50,\n",
       "  250,\n",
       "  152,\n",
       "  101,\n",
       "  25,\n",
       "  13,\n",
       "  65,\n",
       "  315,\n",
       "  123,\n",
       "  124,\n",
       "  110,\n",
       "  316,\n",
       "  9,\n",
       "  113,\n",
       "  311,\n",
       "  317,\n",
       "  318,\n",
       "  44964],\n",
       " [319,\n",
       "  273,\n",
       "  68,\n",
       "  24,\n",
       "  109,\n",
       "  223,\n",
       "  320,\n",
       "  192,\n",
       "  321,\n",
       "  145,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  76,\n",
       "  9,\n",
       "  262,\n",
       "  216,\n",
       "  326,\n",
       "  327,\n",
       "  37,\n",
       "  328,\n",
       "  163,\n",
       "  23,\n",
       "  329,\n",
       "  224,\n",
       "  144,\n",
       "  330,\n",
       "  105,\n",
       "  237,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  163,\n",
       "  153,\n",
       "  41,\n",
       "  334,\n",
       "  152,\n",
       "  50,\n",
       "  85,\n",
       "  251,\n",
       "  335,\n",
       "  26,\n",
       "  113,\n",
       "  9,\n",
       "  24,\n",
       "  27,\n",
       "  336,\n",
       "  90,\n",
       "  29,\n",
       "  24,\n",
       "  337,\n",
       "  9,\n",
       "  338,\n",
       "  122,\n",
       "  123,\n",
       "  339,\n",
       "  101,\n",
       "  26,\n",
       "  91,\n",
       "  340,\n",
       "  85,\n",
       "  12,\n",
       "  341,\n",
       "  342,\n",
       "  201,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  201,\n",
       "  165,\n",
       "  348,\n",
       "  24,\n",
       "  349,\n",
       "  345,\n",
       "  11,\n",
       "  350,\n",
       "  351,\n",
       "  237,\n",
       "  316,\n",
       "  352,\n",
       "  9,\n",
       "  113,\n",
       "  320,\n",
       "  268,\n",
       "  101,\n",
       "  353,\n",
       "  354,\n",
       "  114,\n",
       "  355,\n",
       "  356,\n",
       "  9,\n",
       "  44964],\n",
       " [163,\n",
       "  246,\n",
       "  124,\n",
       "  290,\n",
       "  102,\n",
       "  357,\n",
       "  115,\n",
       "  75,\n",
       "  138,\n",
       "  358,\n",
       "  88,\n",
       "  359,\n",
       "  360,\n",
       "  290,\n",
       "  320,\n",
       "  148,\n",
       "  94,\n",
       "  281,\n",
       "  361,\n",
       "  178,\n",
       "  362,\n",
       "  308,\n",
       "  363,\n",
       "  354,\n",
       "  26,\n",
       "  201,\n",
       "  26,\n",
       "  75,\n",
       "  220,\n",
       "  57,\n",
       "  109,\n",
       "  24,\n",
       "  273,\n",
       "  163,\n",
       "  364,\n",
       "  24,\n",
       "  192,\n",
       "  243,\n",
       "  273,\n",
       "  365,\n",
       "  166,\n",
       "  165,\n",
       "  24,\n",
       "  275,\n",
       "  360,\n",
       "  366,\n",
       "  367,\n",
       "  26,\n",
       "  222,\n",
       "  110,\n",
       "  26,\n",
       "  222,\n",
       "  50,\n",
       "  83,\n",
       "  164,\n",
       "  201,\n",
       "  201,\n",
       "  368,\n",
       "  369,\n",
       "  220,\n",
       "  78,\n",
       "  370,\n",
       "  201,\n",
       "  250,\n",
       "  152,\n",
       "  251,\n",
       "  108,\n",
       "  71,\n",
       "  297,\n",
       "  371,\n",
       "  372,\n",
       "  9,\n",
       "  216,\n",
       "  373,\n",
       "  15,\n",
       "  290,\n",
       "  24,\n",
       "  223,\n",
       "  374,\n",
       "  375,\n",
       "  229,\n",
       "  272,\n",
       "  286,\n",
       "  116,\n",
       "  24,\n",
       "  44962,\n",
       "  106,\n",
       "  325,\n",
       "  376,\n",
       "  377,\n",
       "  105,\n",
       "  44962,\n",
       "  120,\n",
       "  52,\n",
       "  44962,\n",
       "  378,\n",
       "  379,\n",
       "  16,\n",
       "  9,\n",
       "  9,\n",
       "  50,\n",
       "  379,\n",
       "  24,\n",
       "  50,\n",
       "  380,\n",
       "  17,\n",
       "  381,\n",
       "  37,\n",
       "  382,\n",
       "  25,\n",
       "  164,\n",
       "  383,\n",
       "  179,\n",
       "  308,\n",
       "  9,\n",
       "  384,\n",
       "  142,\n",
       "  210,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  24,\n",
       "  337,\n",
       "  9,\n",
       "  24,\n",
       "  389,\n",
       "  390,\n",
       "  156,\n",
       "  391,\n",
       "  389,\n",
       "  24,\n",
       "  114,\n",
       "  392,\n",
       "  275,\n",
       "  393,\n",
       "  72,\n",
       "  9,\n",
       "  394,\n",
       "  395,\n",
       "  337,\n",
       "  9,\n",
       "  223,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  179,\n",
       "  24,\n",
       "  37,\n",
       "  311,\n",
       "  41,\n",
       "  24,\n",
       "  376,\n",
       "  45,\n",
       "  266,\n",
       "  399,\n",
       "  164,\n",
       "  400,\n",
       "  401,\n",
       "  314,\n",
       "  9,\n",
       "  402,\n",
       "  110,\n",
       "  71,\n",
       "  403,\n",
       "  363,\n",
       "  113,\n",
       "  9,\n",
       "  26,\n",
       "  23,\n",
       "  44964],\n",
       " [286,\n",
       "  404,\n",
       "  124,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  297,\n",
       "  44962,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  215,\n",
       "  120,\n",
       "  9,\n",
       "  389,\n",
       "  9,\n",
       "  411,\n",
       "  412,\n",
       "  389,\n",
       "  413,\n",
       "  414,\n",
       "  44964],\n",
       " [405,\n",
       "  414,\n",
       "  273,\n",
       "  415,\n",
       "  416,\n",
       "  416,\n",
       "  416,\n",
       "  417,\n",
       "  199,\n",
       "  416,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  35,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  311,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  115,\n",
       "  430,\n",
       "  73,\n",
       "  68,\n",
       "  121,\n",
       "  431,\n",
       "  178,\n",
       "  432,\n",
       "  389,\n",
       "  424,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  106,\n",
       "  439,\n",
       "  93,\n",
       "  440,\n",
       "  123,\n",
       "  419,\n",
       "  43,\n",
       "  220,\n",
       "  405,\n",
       "  123,\n",
       "  440,\n",
       "  441,\n",
       "  164,\n",
       "  75,\n",
       "  15,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  326,\n",
       "  446,\n",
       "  30,\n",
       "  447,\n",
       "  52,\n",
       "  114,\n",
       "  81,\n",
       "  246,\n",
       "  90,\n",
       "  146,\n",
       "  448,\n",
       "  14,\n",
       "  405,\n",
       "  449,\n",
       "  450,\n",
       "  326,\n",
       "  451,\n",
       "  452,\n",
       "  405,\n",
       "  453,\n",
       "  454,\n",
       "  425,\n",
       "  405,\n",
       "  455,\n",
       "  76,\n",
       "  456,\n",
       "  457,\n",
       "  319,\n",
       "  162,\n",
       "  405,\n",
       "  458,\n",
       "  459,\n",
       "  405,\n",
       "  68,\n",
       "  425,\n",
       "  405,\n",
       "  119,\n",
       "  152,\n",
       "  12,\n",
       "  26,\n",
       "  460,\n",
       "  29,\n",
       "  116,\n",
       "  101,\n",
       "  461,\n",
       "  447,\n",
       "  424,\n",
       "  81,\n",
       "  9,\n",
       "  75,\n",
       "  398,\n",
       "  50,\n",
       "  85,\n",
       "  462,\n",
       "  405,\n",
       "  463,\n",
       "  9,\n",
       "  458,\n",
       "  424,\n",
       "  81,\n",
       "  413,\n",
       "  168,\n",
       "  464,\n",
       "  465,\n",
       "  434,\n",
       "  405,\n",
       "  119,\n",
       "  120,\n",
       "  466,\n",
       "  101,\n",
       "  152,\n",
       "  404,\n",
       "  467,\n",
       "  405,\n",
       "  468,\n",
       "  469,\n",
       "  458,\n",
       "  424,\n",
       "  433,\n",
       "  447,\n",
       "  9,\n",
       "  220,\n",
       "  75,\n",
       "  164,\n",
       "  340,\n",
       "  250,\n",
       "  201,\n",
       "  247,\n",
       "  78,\n",
       "  470,\n",
       "  220,\n",
       "  349,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  405,\n",
       "  57,\n",
       "  124,\n",
       "  476,\n",
       "  470,\n",
       "  101,\n",
       "  477,\n",
       "  194,\n",
       "  478,\n",
       "  479,\n",
       "  424,\n",
       "  175,\n",
       "  78,\n",
       "  480,\n",
       "  481,\n",
       "  9,\n",
       "  5,\n",
       "  482,\n",
       "  389,\n",
       "  164,\n",
       "  247,\n",
       "  405,\n",
       "  483,\n",
       "  484,\n",
       "  29,\n",
       "  26,\n",
       "  175,\n",
       "  485,\n",
       "  72,\n",
       "  55,\n",
       "  486,\n",
       "  28,\n",
       "  424,\n",
       "  487,\n",
       "  9,\n",
       "  90,\n",
       "  216,\n",
       "  76,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  479,\n",
       "  278,\n",
       "  491,\n",
       "  405,\n",
       "  327,\n",
       "  492,\n",
       "  430,\n",
       "  337,\n",
       "  493,\n",
       "  471,\n",
       "  494,\n",
       "  44964]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the length of each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   counts\n",
       "0     659\n",
       "1      95\n",
       "2     171\n",
       "3      22\n",
       "4     209"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lengths(int_texts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistic summary of the length of summaries and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:\n",
      "             counts\n",
      "count  48133.000000\n",
      "mean     409.901793\n",
      "std      995.823531\n",
      "min        2.000000\n",
      "25%       67.000000\n",
      "50%      159.000000\n",
      "75%      391.000000\n",
      "max    46004.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what's the max squence length we can cover by percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887.0\n",
      "1496.0\n",
      "4059.040000000001\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to counts the number of time <UNK appears in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for length limit and number of UNKs\n",
    "\n",
    "Sort the summaries and texts by the length of the element in texts from shortest to longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31981\n",
      "31981\n"
     ]
    }
   ],
   "source": [
    "max_text_length = 886 # This will cover up to 89.5% lengthes\n",
    "min_length = 2\n",
    "unk_text_limit = 1 # text can contain up to 1 UNK word\n",
    "\n",
    "def filter_condition(item):\n",
    "    int_text = item[0]\n",
    "    if (len(int_text) >= min_length and \n",
    "       len(int_text) <= max_text_length and \n",
    "       unk_counter(int_text) <= unk_text_limit):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "int_text_summaries = list(zip(int_texts))\n",
    "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\n",
    "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[0]))\n",
    "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\n",
    "sorted_summaries = list(sorted_int_text_summaries[0])\n",
    "sorted_texts = list(sorted_int_text_summaries[0])\n",
    "# Delete those temporary varaibles\n",
    "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the length of text in sorted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_texts = [len(text) for text in sorted_texts]\n",
    "lengths_texts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__pickleStuff(\"clean_texts.p\",clean_texts)\n",
    "\n",
    "__pickleStuff(\"sorted_summaries.p\",sorted_summaries)\n",
    "__pickleStuff(\"sorted_texts.p\",sorted_texts)\n",
    "__pickleStuff(\"word_embedding_matrix.p\",word_embedding_matrix)\n",
    "\n",
    "__pickleStuff(\"vocab_to_int.p\",vocab_to_int)\n",
    "__pickleStuff(\"int_to_vocab.p\",int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create palceholders for inputs to the model\n",
    "\n",
    "summary_length and text_length are the sentence lengths in a batch, and max_summary_length is the maximum length of a summary in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the last word id from each batch and concatenate the id of <GO the begining of each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):  \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the encoding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bidirectional_dynamic_rnn use tf.variable_scope so that variables are reused with each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters\n",
    "\n",
    "rnn_size: The number of units in the LSTM cell\n",
    "\n",
    "sequence_length: size [batch_size], containing the actual lengths for each of the sequences in the batch\n",
    "\n",
    "num_layers: number of bidirectional RNN layer\n",
    "\n",
    "rnn_inputs: number of bidirectional RNN layer\n",
    "\n",
    "keep_prob: RNN dropout input keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # original code is missing this line below, that is how we connect layers \n",
    "            # by feeding the current layer's output to next layer's input\n",
    "            rnn_inputs = enc_output\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training decoding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters\n",
    "\n",
    "dec_embed_input: output of embedding_lookup for a batch of inputs\n",
    "    \n",
    "summary_length: length of each padded summary sequences in batch, since padded, all lengths should be same number\n",
    "    \n",
    "dec_cell: the decoder RNN cells' output with attention wapper\n",
    "    \n",
    "output_layer: fully connected layer to apply to the RNN output\n",
    "    \n",
    "vocab_size: vocabulary size i.e. len(vocab_to_int)+1\n",
    "    \n",
    "max_summary_length: the maximum length of a summary in a batch\n",
    "    \n",
    "batch_size: number of input sequences in a batch\n",
    "    \n",
    "    \n",
    "Three components\n",
    "\n",
    "TrainingHelper: reads a sequence of integers from the encoding layer.\n",
    "\n",
    "BasicDecoder: processes the sequence with the decoding cell, and an output layer, which is a fully connected layer. initial_state set to zero state.\n",
    "\n",
    "dynamic_decode: creates our outputs that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n",
    "                            vocab_size, max_summary_length,batch_size):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                       helper=training_helper,\n",
    "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                       output_layer = output_layer)\n",
    "\n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create infer decoding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters\n",
    "\n",
    "embeddings: the CN's word_embedding_matrix\n",
    "\n",
    "start_token: the id of <GO\n",
    "\n",
    "end_token: the id of <EOS\n",
    "dec_cell: the decoder RNN cells' output with attention wapper\n",
    "\n",
    "output_layer: fully connected layer to apply to the RNN output\n",
    "\n",
    "max_summary_length: the maximum length of a summary in a batch\n",
    "\n",
    "batch_size: number of input sequences in a batch\n",
    "\n",
    "GreedyEmbeddingHelper argument start_tokens: int32 vector shaped {batch_size, the start tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Decoding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 parts: decoding cell, attention, and getting our logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding Cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a two layer LSTM with dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bhadanau, since trains faster than Luong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AttentionWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applies the attention mechanism to our decoding cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters\n",
    "\n",
    "dec_embed_input: output of embedding_lookup for a batch of inputs\n",
    "\n",
    "embeddings: the CN's word_embedding_matrix\n",
    "\n",
    "enc_output: encoder layer output, containing the forward and the backward rnn output\n",
    "\n",
    "enc_state: encoder layer state, a tuple containing the forward and the backward final states of bidirectional rnn.\n",
    "\n",
    "vocab_size: vocabulary size i.e. len(vocab_to_int)+1\n",
    "\n",
    "text_length: the actual lengths for each of the input text sequences in the batch\n",
    "\n",
    "summary_length: the actual lengths for each of the input summary sequences in the batch\n",
    "\n",
    "max_summary_length: the maximum length of a summary in a batch\n",
    "\n",
    "rnn_size: The number of units in the LSTM cell\n",
    "\n",
    "vocab_to_int: vocab_to_int the dictionary\n",
    "\n",
    "keep_prob: RNN dropout input keep probability\n",
    "\n",
    "batch_size: number of input sequences in a batch\n",
    "\n",
    "num_layers: number of decoder RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                     enc_output,\n",
    "                                                     text_length,\n",
    "                                                     normalize=False,\n",
    "                                                     name='BahdanauAttention')\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size,\n",
    "                                                  max_summary_length,\n",
    "                                                  batch_size)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,\n",
    "                                                    vocab_to_int['<GO>'],\n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell,\n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sentences for batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad so the actual lengths for each of the sequences in the batch have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate batch data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just to test \"get_batches\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a batch with size of 5\n",
    "\n",
    "Checkout those \"44963\" they are PAD>s, also all sequences' lengths are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<PAD>' has id: 44963\n",
      "pad summaries batch samples:\n",
      " [[   26 44964]\n",
      " [23721 44964]\n",
      " [   11 44964]\n",
      " [   14 44964]\n",
      " [   26 44964]]\n"
     ]
    }
   ],
   "source": [
    "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n",
    "sorted_summaries_samples = sorted_summaries[7:50]\n",
    "sorted_texts_samples = sorted_texts[7:50]\n",
    "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n",
    "    sorted_summaries_samples, sorted_texts_samples, 5))\n",
    "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/10 Batch   20/499 - Loss:  9.189, Seconds: 405.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-71-79eda766686c>\", line 35, in <module>\n",
      "    keep_prob: keep_probability})\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\ntpath.py\", line 552, in abspath\n",
      "    return normpath(path)\n",
      "  File \"C:\\Users\\hp\\Anaconda3\\lib\\ntpath.py\", line 474, in normpath\n",
      "    path = os.fspath(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries, sorted_texts, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Our Own Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the quality of the summaries that this model can generate, you can either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_sentences: a list of reviews strings we are going to summarize\n",
    "\n",
    "generagte_summary_length: a int or list, if a list must be same length as input_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from best_model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from best_model.ckpt.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "random = np.random.randint(0,len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"best_model.ckpt.data-00000-of-00001\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph('best_model.ckpt.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
